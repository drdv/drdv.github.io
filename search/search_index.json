{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#welcome-to-my-page","title":"Welcome to my page!","text":"<p>I have worked as a senior data scientist at Kelkoo (France) and a data scientist/software engineer at ProbaYes (France). I obtained a Ph. D. degree in 2006 from Tohoku University (Japan) and a M. Sc. diploma in 1999 from the Technical University-Sofia (Bulgaria). I have been a research scientist at INRIA - Grenoble (France), and a research scientist / lecturer at AASS \u00d6rebro University (Sweden).</p>"},{"location":"#contact","title":"Contact","text":"<ul> <li>email: mail.mitko@gmail.com</li> </ul>"},{"location":"cv/","title":"CV","text":""},{"location":"cv/#curriculum-vitae","title":"Curriculum vitae","text":"<p>I was born in Sofia, Bulgaria (20.11.1975)</p>"},{"location":"cv/#professional-experience","title":"Professional experience","text":"2021.03 - 2024.05: Kelkoo <p>senior data scientist (science team)</p> researchcoding <ul> <li>machine learning and operations research for online bidding</li> <li>traffic / resource optimization</li> <li>modeling of merchant-publisher interaction (handling of sparse data)</li> <li>multi-objective formulations for margin optimization</li> </ul> <ul> <li><code>python</code><ul> <li>development of company standards for code quality and documentation</li> <li>design/maintenance of common libraries</li> <li>put in place a local PyPi server</li> </ul> </li> <li><code>scala</code><ul> <li>production quality code</li> </ul> </li> <li><code>spark</code><ul> <li>production quality code</li> </ul> </li> <li>working with <code>HDFS</code>, <code>YARN</code></li> <li>in charge of scientific and project documentation</li> </ul> 2017.09 - 2021.03: ProbaYes <p>data scientist/software engineer</p> researchcoding <ul> <li> <p>machine learning applied to various domains</p> <ul> <li>fault detection</li> <li>image classification (deep learning)</li> <li>deep reinforcement learning for trajectory generation</li> <li>camera calibration</li> </ul> </li> <li> <p>causal analysis</p> </li> <li>concurrent HMM for behavior detection</li> <li>graph analysis</li> <li>time series analysis</li> <li>evaluation of fuzzy controllers</li> <li>development of strategies for fast evaluation of boolean functions<ul> <li>using techniques employed in compiler optimization</li> <li>application: quantum computing</li> </ul> </li> </ul> <ul> <li><code>python</code><ul> <li>delivered code for many projects to clients (on Linux/Windows)</li> </ul> </li> <li><code>C++</code><ul> <li>worked as consultant/contractor on code base of Valeo</li> <li>design/maintenance of <code>CMake</code> based build systems</li> </ul> </li> <li><code>rust</code><ul> <li>developed rocket simulator for MBDA</li> </ul> </li> </ul>"},{"location":"cv/#research-experience","title":"Research experience","text":"2012.10 - 2015.12: INRIA Rh\u00f4ne-Alpes <p>research scientist (BIPOP team)</p> researchcoding <ul> <li>fast and reliable solution of lexicographic optimization problems applied to robot control<ul> <li>implementation of solver in <code>C++</code></li> <li>applied to real-world robot safety</li> </ul> </li> <li>nonlinear model predictive control</li> <li>robust control for safe locomotion</li> <li>minimum-time control</li> <li>simulation of mechanical systems</li> <li>numerical analysis</li> </ul> <ul> <li><code>c++</code><ul> <li>implemented an active-set solver for lexicographic least-squares problems</li> </ul> </li> <li><code>matlab</code></li> </ul> 2007.12 - 2012.09: \u00d6rebro University <p>research scientist/lecturer (Mobile Robotics and Olfaction Lab at AASS)</p> researchteaching <ul> <li>model predictive control applied to walking robots</li> <li>robotic grasping</li> <li>motion planning for nonholonomic vehicles</li> <li>combining task and motion planning for robotic manipulators</li> </ul> <ul> <li>control of linear &amp; nonlinear systems</li> <li>simulation of multibody systems</li> <li>introduction to optimization</li> </ul> 2006.12 - 2007.12: INRIA Rh\u00f4ne-Alpes <p>research scientist (BIPOP team)</p> <ul> <li>model predictive control for biped robot walking motion generation</li> </ul>"},{"location":"cv/#education","title":"Education","text":"2003.04 - 2006.03: Tohoku University <ul> <li>Space Robotics Laboratory Department of Aerospace Engineering</li> <li>degree: Ph.D. in aerospace engineering</li> <li>topic: dynamics and control of space manipulators during a satellite capturing operation</li> <li>advisor: Prof. Kazuya Yoshida</li> <li>awards: Scholarship from the Japanese Ministry of Education MONBU-SHYO (2003.04 - 2006.03)</li> </ul> 2003.04 - 2006.03: Hirosaki University <ul> <li>Department of Mechanical Science and Engineering</li> <li>advisor: Prof. Dragomir Nenchev</li> <li>awards: Scholarship from Hirosaki University (2002.10 - 2003.03)</li> </ul> 1994.09 - 2001.09: Technical University - Sofia <ul> <li>Faculty of Automatics</li> <li>degrees: M.Sc. in robotics (1999.09), B.A. in electrical engineering (1997.09)</li> <li>advisor: Prof. Veselin Pavlov (M.Sc. thesis)</li> <li>I decided to move to Japan after having spent two years as a doctoral student</li> </ul>"},{"location":"cv/#publications","title":"Publications","text":"<p>Link to my publications</p>"},{"location":"cv/#languages","title":"Languages","text":"<ul> <li>Bulgarian (native)</li> <li>English (fluent)</li> <li>Japanese (conversational)</li> <li>Russian (basic)</li> <li>French (beginner)</li> </ul>"},{"location":"gpg/","title":"GPG","text":""},{"location":"gpg/#my-public-gpg-key","title":"My public GPG key","text":"<p>download</p> curl -O https://drdv.github.io/snippets/gpg.pub<pre><code>-----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmDMEZ458khYJKwYBBAHaRw8BAQdAkBhddFhcRYs1EKx1jW64JFtEREFvnFbvcj3z\njUfqb5i0J0RpbWl0YXIgRGltaXRyb3YgPG1haWwubWl0a29AZ21haWwuY29tPoiZ\nBBMWCgBBFiEERWMPNi9Au2CuA4r4aRFddezX+AMFAmeOfJICGwMFCQHhM4AFCwkI\nBwICIgIGFQoJCAsCBBYCAwECHgcCF4AACgkQaRFddezX+AP1oQEAoryf2/47LDZg\nyUjJPEDRBLJR28JEYu1YgEOfV7DLOM4A/j9Unk2RKqrT7X+4LUnsBgaltYFiv8OJ\nVZTXR2KA/TEKuDgEZ458khIKKwYBBAGXVQEFAQEHQO+xF/Dz4/ssO1BnL763tonh\nO/7oIeCjEuq1Fswn+p8aAwEIB4h+BBgWCgAmFiEERWMPNi9Au2CuA4r4aRFddezX\n+AMFAmeOfJICGwwFCQHhM4AACgkQaRFddezX+AOwJgD/cClTfav+hIJbwx079SJN\nLT3oAMtE/R0h14eJcgdadI0A/RDXSZ+WRWIMnjzQ9qTcTcgpxUd4mBpDRfkOvfNV\ng0YPuDMEZ459gxYJKwYBBAHaRw8BAQdA/AGlSd//ZR02mtknvYZi8lK+mg9OeU+q\nvr/qjnQAkK2I9QQYFgoAJhYhBEVjDzYvQLtgrgOK+GkRXXXs1/gDBQJnjn2DAhsC\nBQkB4TOAAIEJEGkRXXXs1/gDdiAEGRYKAB0WIQRBzsYKnvZiN9ZD7PGmQDcPzRW6\n2gUCZ459gwAKCRCmQDcPzRW62gOTAQDY4yb9Ad9Pyb/8TQsxe8SjTyrhYEF8O6TF\nu/zi0iyjngD6AuN5uNWwDLGXzkeArxQuNBWugC6xlrReCL0plii23gk1sQEA98dH\nntFVYlT7+87qNyCd3fhLneXEfrpKD9jJevKOIwkA/2sz6lAHY8cxMPRhbM05qSNG\nch2V4fD6ZOraVgfP1NEIuDMEZ459sRYJKwYBBAHaRw8BAQdAe0thSiLgmVqp8nkC\nyxgRZ3qTomYLk1krcnLEz9URjJuIfgQYFgoAJhYhBEVjDzYvQLtgrgOK+GkRXXXs\n1/gDBQJnjn2xAhsgBQkB4TOAAAoJEGkRXXXs1/gDamEBAABCvtB3ma8wqc3DJrgZ\nDTRrdnjBAymPR1of7Kc30rUkAQDYB81P8tZK65w5bXWbsvjCNiJuv32X74ZyQYAq\nNosEDw==\n=X9hb\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre> gpg --show-keys --fingerprint --keyid-format=long gpg.pub<pre><code>pub   ed25519/69115D75ECD7F803 2025-01-20 [SC] [expires: 2026-01-20]\n      Key fingerprint = 4563 0F36 2F40 BB60 AE03  8AF8 6911 5D75 ECD7 F803\nuid                            Dimitar Dimitrov &lt;mail.mitko@gmail.com&gt;\nsub   cv25519/731BF89E76B1F7FA 2025-01-20 [E] [expires: 2026-01-20]\nsub   ed25519/A640370FCD15BADA 2025-01-20 [S] [expires: 2026-01-20]\nsub   ed25519/C82ACD166A68F0DF 2025-01-20 [A] [expires: 2026-01-20]\n</code></pre>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#publications","title":"Publications","text":""},{"location":"publications/#2016","title":"2016","text":"Efficient resolution of potentially conflicting linear constraints in robotics <p>manuscript</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Alexander Sherikov</li> <li>Pierre-Brice Wieber</li> </ul> <ul> <li>not published</li> </ul> Geometric and numerical aspects of redundancy <p>preprint</p> authorsbibtex <ul> <li>Pierre-Brice Wieber</li> <li>Adrien Escande</li> <li>Dimitar Dimitrov</li> <li>Alexander Sherikov</li> </ul> <pre><code>@incollection {Wieber.2016,\n author     = {Wieber, Pierre-Brice and Escande, Adrien and Dimitrov, Dimitar and Sherikov, Alexander},\n title      = {Geometric and numerical aspects of redundancy},\n editor     = {J. P. Laumond et al.},\n booktitle  = {Geometric and Numerical Foundations of Movements},\n publisher  = {Springer-Verlag},\n pages      = {67--85},\n year       = 2016}\n</code></pre> Safe navigation strategies for a biped robot walking in a crowd <p>preprint video</p> authorsbibtex <ul> <li>Nestor Bohorquez</li> <li>Alexander Sherikov</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Bohorquez.2016,\n author     = {Bohorquez, Nestor and Sherikov, Alexander and Dimitrov, Dimitar and Wieber, Pierre-Brice},\n title      = {Safe navigation strategies for a biped robot walking in a crowd},\n booktitle  = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},\n pages      = {379--386},\n year       = 2016}\n</code></pre> A newton method with always feasible iterates for nonlinear model predictive control of walking in a multi-contact situation <p>preprint video</p> authorsbibtex <ul> <li>Diana Serra</li> <li>Camille Brasseur</li> <li>Alexander Sherikov</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Serra.2016,\n author     = {Serra, Diana and Brasseur, Camille and Sherikov, Alexander and Dimitrov, Dimitar and Wieber, Pierre-Brice},\n title      = {A newton method with always feasible iterates for nonlinear model predictive control of walking in a multi-contact situation},\n booktitle  = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},\n pages      = {932--937},\n year       = 2016}\n</code></pre> A hierarchical approach to minimum-time control of industrial robots <p>preprint</p> authorsbibtex <ul> <li>Saed Al Homsi</li> <li>Alexander Sherikov</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Holmsi.2016,\n author     = {Al~Homsi, Saed and Sherikov, Alexander and Dimitrov, Dimitar and Wieber, Pierre-Brice},\n title      = {A hierarchical approach to minimum-time control of industrial robots},\n booktitle  = {IEEE International Conference on Robotics and Automation (ICRA)},\n pages      = {2368--2374},\n year       = 2016}\n</code></pre>"},{"location":"publications/#2015","title":"2015","text":"Autonomous transport vehicles: where we are and what is missing <p>paper</p> authorsbibtex <ul> <li>Henrik Andreasson</li> <li>Abdelbaki Bouguerra</li> <li>Marcello Cirillo</li> <li>Dimitar Dimitrov</li> <li>Dimiter Driankov</li> <li>Lars Karlsson</li> <li>Achim J. Lilienthal</li> <li>Federico Pecora</li> <li>Jari Saarinen</li> <li>Alexander Sherikov</li> <li>Todor Stoyanov</li> </ul> <pre><code>@article      {Andreasson.2015,\n author     = {Andreasson, Henrik and Bouguerra, Abdelbaki and Cirillo, Marcello and Dimitrov, Dimitar and Driankov, Dimiter and Karlsson, Lars and et al.},\n title      = {Autonomous transport vehicles: where we are and what is missing},\n journal    = {IEEE Robotics and Automation Magazine (IEEE-RAM)},\n volume     = {22},\n number     = {1},\n pages      = {64--75},\n year       = 2015}\n</code></pre> Model predictive motion control based on generalized dynamical movement primitives <p>preprint</p> authorsbibtex <ul> <li>Robert Krug</li> <li>Dimitar Dimitrov</li> </ul> <pre><code>@article      {Krug.2015,\n author     = {Krug, Robert and Dimitrov, Dimitar},\n title      = {Model predictive motion control based on generalized dynamical movement primitives},\n journal    = {Journal of Intelligent \\&amp; Robotic Systems},\n volume     = {77},\n number     = {1},\n pages      = {17--35},\n year       = 2015}\n</code></pre> Balancing a humanoid robot with a prioritized contact force distribution <p>preprint video</p> authorsbibtex <ul> <li>Alexander Sherikov</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Sherikov.2015,\n author     = {Sherikov, Alexander and Dimitrov, Dimitar and Wieber, Pierre-Brice},\n title      = {Balancing a humanoid robot with a prioritized contact force distribution},\n booktitle  = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},\n pages      = {223--228},\n year       = 2015}\n</code></pre> A robust linear MPC approach to online generation of 3D biped walking motion <p>preprint video</p> authorsbibtex <ul> <li>Camille Brasseur</li> <li>Alexander Sherikov</li> <li>Cyrille Collette</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Brasseur.2015,\n author     = {Brasseur, Camille and Sherikov, Alexander and Collette, Cyrille and Dimitrov, Dimitar and Wieber, Pierre-Brice},\n title      = {A robust linear MPC approach to online generation of 3D biped walking motion},\n booktitle  = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},\n pages      = {595--601},\n year       = 2015}\n</code></pre>"},{"location":"publications/#2014","title":"2014","text":"Efficiently combining task and motion planning using geometric constraints <p>preprint</p> authorsbibtex <ul> <li>Fabien Lagriffoul</li> <li>Dimitar Dimitrov</li> <li>Julien Bidot</li> <li>Alessandro Saffiotti</li> <li>Lars Karlsson</li> </ul> <pre><code>@article      {Lagriffoul.2014,\n author     = {Lagriffoul, Fabien and Dimitrov, Dimitar and Bidot, Julien and Saffiotti, Alessandro and Karlsson, Lars},\n title      = {Efficiently combining task and motion planning using geometric constraints},\n journal    = {The International Journal of Robotics Research},\n volume     = {33},\n number     = {14},\n pages      = {1726--1747},\n year       = 2014}\n</code></pre> Whole body motion controller with long-term balance constraints <p>preprint video</p> authorsbibtex <ul> <li>Alexander Sherikov</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Sherikov.2014,\n author     = {Sherikov, Alexander and Dimitrov, Dimitar and Wieber, Pierre-Brice},\n title      = {Whole body motion controller with long-term balance constraints},\n booktitle  = {IEEE-RAS International Conference on Humanoid Robots (Humanoids)},\n pages      = {444--450},\n year       = 2014}\n</code></pre> Multi-objective control of robots <p>preprint</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> <li>Adrien Escande</li> </ul> <pre><code>@article      {Dimitrov.2014,\n author     = {Dimitrov, Dimitar and Wieber, Pierre-Brice and Escande, Adrien},\n title      = {Multi-objective control of robots},\n journal    = {Journal of the Robotics Society of Japan},\n volume     = {32},\n number     = {6},\n pages      = {512--518},\n year       = 2014}\n</code></pre>"},{"location":"publications/#2013","title":"2013","text":"Representing movement primitives as implicit dynamical systems learned from multiple demonstrations <p>preprint</p> authorsbibtex <ul> <li>Robert Krug</li> <li>Dimitar Dimitrov</li> </ul> <pre><code>@inproceedings{Krug.2013,\n author     = {Krug, Robert and Dimitrov, Dimitar},\n title      = {Representing movement primitives as implicit dynamical systems learned from multiple demonstrations},\n booktitle  = {International Conference on Advanced Robotics (ICAR)},\n pages      = {1--8},\n year       = 2013}\n</code></pre>"},{"location":"publications/#2012","title":"2012","text":"Constraint propagation on interval bounds for dealing with geometric backtracking <p>preprint</p> authorsbibtex <ul> <li>Fabien Lagriffoul</li> <li>Dimitar Dimitrov</li> <li>Alessandro Saffiotti</li> <li>Lars Karlsson</li> </ul> <pre><code>@inproceedings{Lagriffoul.2012,\n author     = {Lagriffoul, Fabien and Dimitrov, Dimitar and Saffiotti, Alessandro and Karlsson, Lars},\n title      = {Constraint propagation on interval bounds for dealing with geometric backtracking},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {957--964},\n year       = 2012}\n</code></pre> On mission-dependent coordination of multiple vehicles under spatial and temporal constraints <p>preprint</p> authorsbibtex <ul> <li>Federico Pecora</li> <li>Marcello Cirillo</li> <li>Dimitar Dimitrov</li> </ul> <pre><code>@inproceedings{Pecora.2012,\n author     = {Pecora, Federico and Cirillo, Marcello and Dimitrov, Dimitar},\n title      = {On mission-dependent coordination of multiple vehicles under spatial and temporal constraints},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {5262--5269},\n year       = 2012}\n</code></pre> Independent contact regions based on a patch contact model <p>preprint</p> authorsbibtex <ul> <li>Krzysztof Charusta</li> <li>Robert Krug</li> <li>Dimitar Dimitrov</li> <li>Boyko Iliev</li> </ul> <pre><code>@inproceedings{Charusta.2012b,\n author     = {Charusta, Krzysztof and Krug, Robert and Dimitrov, Dimitar and Iliev, Boyko},\n title      = {Independent contact regions based on a patch contact model},\n booktitle  = {IEEE International Conference on Robotics and Automation (ICRA)},\n pages      = {4162--4169},\n year       = 2012}\n</code></pre> Generation of independent contact regions on objects reconstructed from noisy real-world range data <p>preprint</p> authorsbibtex <ul> <li>Krzysztof Charusta</li> <li>Robert Krug</li> <li>Todor Stoyanov</li> <li>Dimitar Dimitrov</li> <li>Boyko Iliev</li> </ul> <pre><code>@inproceedings{Charusta.2012a,\n author     = {Charusta, Krzysztof and Krug, Robert and Stoyanov, Todor and Dimitrov, Dimitar and Iliev, Boyko},\n title      = {Generation of independent contact regions on objects reconstructed from noisy real-world range data},\n booktitle  = {IEEE International Conference on Robotics and Automation (ICRA)},\n pages      = {1338--1344},\n year       = 2012}\n</code></pre> Mapping between different kinematic structures without absolute positioning during operation <p>paper</p> authorsbibtex <ul> <li>Erik Berglund</li> <li>Boyko Iliev</li> <li>Rainer Palm</li> <li>Robert Krug</li> <li>Krzysztof Charusta</li> <li>Dimitar Dimitrov</li> </ul> <pre><code>@article      {Berglund.2012,\n author     = {Berglund, Erik and Iliev, Boyko and Palm, Rainer and Krug, Robert and Charusta, Krzysztof and Dimitrov, Dimitar},\n title      = {Mapping between different kinematic structures without absolute positioning during operation},\n journal    = {Electronics Letters},\n volume     = {48},\n number     = {18},\n pages      = {1110--1112},\n year       = 2012}\n</code></pre>"},{"location":"publications/#2011","title":"2011","text":"A sparse model predictive control formulation for walking motion generation <p>preprint presentation errata implementation</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Alexander Sherikov</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Dimitrov.2011b,\n author     = {Dimitrov, Dimitar and Sherikov, Alexander and Wieber, Pierre-Brice},\n title      = {A sparse model predictive control formulation for walking motion generation},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {2292--2299},\n year       = 2011}\n</code></pre> Prioritized independent contact regions for form closure grasps <p>preprint</p> authorsbibtex <ul> <li>Robert Krug</li> <li>Dimitar Dimitrov</li> <li>Krzysztof Charusta</li> <li>Boyko Iliev</li> </ul> <pre><code>@inproceedings{Krug.2011,\n author     = {Krug, Robert and Dimitrov, Dimitar and Charusta, Krzysztof and Iliev, Boyko},\n title      = {Prioritized independent contact regions for form closure grasps},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {1797--1803},\n year       = 2011}\n</code></pre> Walking motion generation with online foot position adaptation based on \\(\\ell_1\\)- and \\(\\ell_\\infty\\)-norm penalty formulations <p>preprint presentation</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Antonio Paolillo</li> <li>Pierre-Brice Wieber</li> </ul> <pre><code>@inproceedings{Dimitrov.2011a,\n author     = {Dimitrov, Dimitar and Paolillo, Antonio and Wieber, Pierre-Brice},\n title      = {Walking motion generation with online foot position adaptation based on $\\ell_1$- and $\\ell_\\infty$-norm penalty formulations},\n booktitle  = {IEEE International Conference on Robotics and Automation (ICRA)},\n pages      = {3523--3529},\n year       = 2011}\n</code></pre>"},{"location":"publications/#2010","title":"2010","text":"Online walking motion generation with automatic foot step placement <p>preprint</p> authorsbibtex <ul> <li>Andrei Herdt</li> <li>Holger Diedam</li> <li>Pierre-Brice Wieber</li> <li>Dimitar Dimitrov</li> <li>Katja Mombaur</li> <li>Moritz Diehl</li> </ul> <pre><code>@article      {Herdt.2010,\n author     = {Herdt, Andrei and Diedam, Holger and Wieber, Pierre-Brice and Dimitrov, Dimitar and Mombaur, Katja and Diehl, Moritz},\n title      = {Online walking motion generation with automatic foot step placement},\n journal    = {Advanced Robotics},\n volume     = {24},\n number     = {5--6},\n pages      = {719--737},\n year       = 2010}\n</code></pre> On the efficient computation of independent contact regions for force closure grasps <p>preprint</p> authorsbibtex <ul> <li>Robert Krug</li> <li>Dimitar Dimitrov</li> <li>Krzysztof Charusta</li> <li>Boyko Iliev</li> </ul> <pre><code>@inproceedings{Krug.2010,\n author     = {Krug, Robert and Dimitrov, Dimitar and Charusta, Krzysztof and Iliev, Boyko},\n title      = {On the efficient computation of independent contact regions for force closure grasps},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {586--591},\n year       = 2010}\n</code></pre> An optimized linear model predictive control solver <p>paper</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> <li>Olivier Stasse,</li> <li>Hans Joachim Ferreau</li> <li>Holger Diedam</li> </ul> <pre><code>@incollection {Dimitrov.2010,\n author     = {Dimitrov, Dimitar and Wieber, Pierre-Brice and Stasse, Olivier and Ferreau, Hans Joachim and Diedam, Holger},\n title      = {An optimized linear model predictive control solver},\n editor     = {Diehl, Moritz and Glineur, Fran\\c{c}ois and Jarlebring, Elias and Michiels, Wim},\n booktitle  = {Recent Advances in Optimization and its Applications in Engineering},\n publisher  = {Springer},\n pages      = {309--318},\n year       = 2010}\n</code></pre>"},{"location":"publications/#2009","title":"2009","text":"An optimized linear model predictive control solver for online walking motion generation <p>paper</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> <li>Olivier Stasse,</li> <li>Hans Joachim Ferreau</li> <li>Holger Diedam</li> </ul> <pre><code>@inproceedings{Dimitrov.2009,\n author     = {Dimitrov, Dimitar and Wieber, Pierre-Brice and Stasse, Olivier and Ferreau, Hans Joachim and Diedam, Holger},\n title      = {An optimized linear model predictive control solver for online walking motion generation},\n booktitle  = {IEEE International Conference on Robotics and Automation (ICRA)},\n pages      = {1171--1176},\n year       = 2009}\n</code></pre> Extraction of grasp related features by human dual-hand object exploration <p>paper</p> authorsbibtex <ul> <li>Krzysztof Charusta</li> <li>Dimitar Dimitrov</li> <li>Achim J. Lilienthal</li> <li>Boyko Iliev</li> </ul> <pre><code>@inproceedings{Charusta.2009,\n author     = {Charusta, Krzysztof and Dimitrov, Dimitar and Lilienthal, Achim J and Iliev, Boyko},\n title      = {Extraction of grasp related features by human dual-hand object exploration},\n booktitle  = {International Conference on Advanced Robotics (ICAR)},\n pages      = {122--127},\n year       = 2009}\n</code></pre>"},{"location":"publications/#2008","title":"2008","text":"Online walking gait generation with adaptive foot positioning through linear model predictive control <p>paper</p> authorsbibtex <ul> <li>Holger Diedam</li> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> <li>Katja Mombaur</li> <li>Moritz Diehl</li> </ul> <pre><code>@inproceedings{Diedam.2008,\n author     = {Diedam, Holger and Dimitrov, Dimitar and Wieber, Pierre-Brice and Mombaur, Katja and Diehl, Moritz},\n title      = {Online walking gait generation with adaptive foot positioning through linear model predictive control},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {1121--1126},\n year       = 2008}\n</code></pre> On the implementation of model predictive control for on-line walking pattern generation <p>paper</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Pierre-Brice Wieber</li> <li>Hans Joachim Ferreau</li> <li>Moritz Diehl</li> </ul> <pre><code>@inproceedings{Dimitrov.2008,\n author     = {Dimitrov, Dimitar and Wieber, Pierre-Brice and Ferreau, Hans Joachim and Diehl, Moritz},\n title      = {On the implementation of model predictive control for on-line walking pattern generation},\n booktitle  = {IEEE International Conference on Robotics and Automation (ICRA)},\n pages      = {2685--2690},\n year       = 2008}\n</code></pre>"},{"location":"publications/#2006","title":"2006","text":"On the capture of tumbling satellite by a space robot <p>paper</p> authorsbibtex <ul> <li>Kazuya Yoshida</li> <li>Dimitar Dimitrov</li> <li>Hiroki Nakanishi</li> </ul> <pre><code>@inproceedings{Yoshida.2006,\n author     = {Yoshida, Kazuya and Dimitrov, Dimitar and Nakanishi, Hiroki},\n title      = {On the capture of tumbling satellite by a space robot},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {4127--4132},\n year       = 2006}\n</code></pre> Utilization of holonomic distribution control for reactionless path planning <p>paper</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Kazuya Yoshida</li> </ul> <pre><code>@inproceedings{Dimitrov.2006,\n author     = {Dimitrov, Dimitar and Yoshida, Kazuya},\n title      = {Utilization of holonomic distribution control for reactionless path planning},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {3387--3392},\n year       = 2006}\n</code></pre> Dynamics and control of space manipulators during a satellite capturing operation <p>thesis</p> authorsbibtex <ul> <li>Dimitar Dimitrov<ul> <li>Tohoku University (Japan), Space Robotics Laboratory</li> </ul> </li> </ul> <pre><code>@phdthesis    {Dimitrov.thesis.2006,\n author     = {Dimitrov, Dimitar},\n title      = {Dynamics and control of space manipulators during a satellite capturing operation},\n school     = {Tohoku University},\n year       = 2006}\n</code></pre>"},{"location":"publications/#2005","title":"2005","text":"Utilization of distributed momentum control for planning approaching trajectories of a space manipulator to a target satellite <p>preprint</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Kazuya Yoshida</li> </ul> <pre><code>@inproceedings{Dimitrov.2005,\n author     = {Dimitrov, Dimitar and Yoshida, Kazuya},\n title      = {Utilization of distributed momentum control for planning approaching trajectories of a space manipulator to a target satellite},\n booktitle  = {International Symposium on Artificial Intelligence, Robotics and Automation in Space (i-SAIRAS)},\n year       = 2005}\n</code></pre>"},{"location":"publications/#2004","title":"2004","text":"Utilization of the bias momentum approach for capturing a tumbling satellite <p>preprint</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Kazuya Yoshida</li> </ul> <pre><code>@inproceedings{Dimitrov.2004b,\n author     = {Dimitrov, Dimitar and Yoshida, Kazuya},\n title      = {Utilization of the bias momentum approach for capturing a tumbling satellite},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {3333--3338},\n year       = 2004}\n</code></pre> Momentum distribution in a space manipulator for facilitating the post-impact control <p>preprint</p> authorsbibtex <ul> <li>Dimitar Dimitrov</li> <li>Kazuya Yoshida</li> </ul> <pre><code>@inproceedings{Dimitrov.2004a,\n author     = {Dimitrov, Dimitar and Yoshida, Kazuya},\n title      = {Momentum distribution in a space manipulator for facilitating the post-impact control},\n booktitle  = {IEEE/RSJ International Conference on Intelligent Robots and System (IROS)},\n pages      = {3345--3350},\n year       = 2004}\n</code></pre>"},{"location":"blog/","title":"Posts","text":""},{"location":"blog/#posts","title":"Posts","text":""},{"location":"blog/202411-summer-walking-challenge/","title":"202411-summer-walking-challenge","text":""},{"location":"blog/202411-summer-walking-challenge/#202411-summer-walking-challenge","title":"202411-summer-walking-challenge","text":"<ul> <li><code>data/export_clean.csv</code> is all the data I need (it is a post-processed extract of the   data exported from my iphone).</li> </ul>"},{"location":"blog/202412-python-strings/","title":"202412-python-strings","text":""},{"location":"blog/202412-python-strings/#202412-python-strings","title":"202412-python-strings","text":"<p>No artifacts need to be generated here</p> <ul> <li><code>emoji.py</code>: Download and visualize emoji (I just needed to select several nice cases   for the post).</li> <li><code>verify_string_encoding.py</code>: Numerical verification that I have understood PEP393 and   the CPython code (not nice, but had to be done!). In one of his lectures Stephen Boyd   said that sometimes we have to test things in a way we are not prowd of - we should do   it, delete the code and never admit what happened.</li> </ul>"},{"location":"blog/202511-makefile-doc/","title":"20251101-makefile-doc","text":""},{"location":"blog/202511-makefile-doc/#20251101-makefile-doc","title":"20251101-makefile-doc","text":"<p>The examples are generated manually directly in the <code>makefile-doc</code> repo at tag <code>v1.4</code>.</p>"},{"location":"blog/202411-summer-walking-challenge/","title":"Summer walking challenge","text":""},{"location":"blog/202411-summer-walking-challenge/#summer-walking-challenge","title":"Summer walking challenge","text":"<p>Walking has always been an important part of my routine. This summer I decided to collect some data. Here are the results ...</p> <p></p> <p>Figure 1. Daily distance (entire challenge)</p> <p></p> <p>Figure 2. Daily distance (post-challenge)</p> <p></p> <p>Figure 3. Daily distance (last month of challenge)</p> <p></p> <p>Figure 4. Daily step count (last month of challenge)</p>"},{"location":"blog/202411-summer-walking-challenge/#the-challenge","title":"The challenge","text":"<p>On May 21st, I decided to consistently carry my phone with me while walking, not with the intention of walking more than usual, but simply to satisfy my curiosity and track the distance. As it turned out, however, I was on a journey to prove, yet again, the Heisenberg Uncertainty Principle (that observation alters the phenomenon being observed). Before long, I had set targets for myself, and 85 days later, I had walked 1900 km.</p> <p>Figure 1 above shows the distance walked per day (in km), along with the dates when I bought a new pair of walking shoes and when I had to discard them. Figures 3 &amp; 4 zoom in on the final month of the challenge, during which I averaged 28 km per day (roughly 45K steps). It's interesting to observe my pace in the 85 days following the end of the challenge, as shown in Figure 2 (clearly, I couldn't reduce my walking right away).</p> <p>By far the most interesting for me is Figure 5, every point on which represents average distance walked (right axis) and total distance covered (left axis) in the past 31 days. For example, the first point indicates that I have walked 500 km between May 21st and June 20th (which is around 16 km per day on average). The point on August 12th is a summary of what is depicted in Figure 3 and so on. The linear increase in pace is something I didn't aim for. I remember challenging myself to reach, at first, 20 km for the past month, later the target moved to 25 and towards the end 28. I briefly considered pushing for an average of 30 km, but after discarding my walking shoes on August 13th (which, by that point, were in terrible condition), I had difficulty adjusting to a new pair. I also felt tired, so I decided to end the challenge. Adding just two more km per day may not seem like a big deal but, trust me, it requires a level of consistency that's tough to maintain, while my kids kept insisting to go camping. Figure 6 is similar to Figure 5, but the rolling period is one week instead of 31 days (as can be seen, I did manage to hit an average of 30 km per day over the course of a week).</p> <p>Somewhere in the middle of all this, I aimed to cover a marathon distance in a single day, but my daily maximum ended up being around 36 km.</p> <p></p> <p>Figure 5. 31-day rolling distance (entire challenge)</p> <p></p> <p>Figure 6. 7-day rolling distance (entire challenge)</p>"},{"location":"blog/202411-summer-walking-challenge/#a-typical-day","title":"A typical day","text":"<p>I wake up around 6:30 and start the day with about half an hour of reading. Then I stretch and take a few moments to plan what I want to accomplish, aside from my walks. I have breakfast at 7:30, then head out for my first walk of the day at 8:00, which typically lasts about an hour and a half. From 10:00 to 11:00, I focus on other tasks, then have an apple and go for another walk, usually lasting an hour. Lunch is around 12:30 and by 14:00 I'm out again until about 15:00. Afterwards, I take a 30-minute nap and work on other tasks until 17:00 when I have another apple and head out for one more hour. Dinner is around 18:30, followed by my longest walk of the day, which typically lasts about two hours.</p> <p>I mostly walk on flat terrain but occasionally I go hiking. There are three outdoor exercise parks near my place, and I pass by one almost every day. I usually stop to do push-ups and pull-ups, which fit perfectly into my walking routine.</p> <p>All in all, this adds up to between 4 and 7 hours of walking per day.</p>"},{"location":"blog/202411-summer-walking-challenge/#lessons-learned","title":"Lessons learned","text":"<ul> <li>Nutrition plays a key role in sustaining such an effort over time. I had to pay close   attention especially to my protein intake in order to avoid loosing too much weight.   Although I normally don't eat much meat, I found myself craving it daily - and, of   course, I ate.</li> <li>Combining walking with strength training makes me feel amazing - just walking alone   isn't enough.</li> <li>During the challenge I used my bike only once (and I don't own a car), so much like   Forrest Gump, if I was going somewhere - I was walking. This gave me time to reflect   on ideas and communicate with people. Communication while walking is quite a bit more   pleasant for me compared to doing it while e.g., eating at a restaurant.</li> <li>Walking 28 km per day isn't really necessary - 10 to 12 km is probably enough to reap   most of the benefits. However, there's something magical about finding your rhythm   and completely dedicating yourself to it.</li> <li>It is possible to meditate while walking.</li> <li>Overall, I found my early morning walk to be the most beneficial, as it set my mood   for the day, while my after-dinner walk helped me calm down and improved my sleep.</li> <li>Walking helped me with my knees.</li> <li>I found myself walking for three reasons: (1) because I needed it badly, (2) because I   had to go somewhere, and (3) because I wanted to hit a target distance. The latter one   doesn't seem very meaningful now.</li> <li>It would have been better to use at least two pairs of good walking shoes. The ones I   bought weren't well-suited for walking on asphalt.</li> </ul>"},{"location":"blog/202411-summer-walking-challenge/#data","title":"Data","text":"<p>Here is the csv data used to generate the above figures.</p> <p></p>"},{"location":"blog/202412-python-strings/","title":"Anatomy of python strings","text":""},{"location":"blog/202412-python-strings/#anatomy-of-python-strings","title":"Anatomy of python strings","text":"<p>From the docs: \"Strings are immutable sequences of Unicode code points\". This requires a bit of unpacking ...</p>"},{"location":"blog/202412-python-strings/#terminology","title":"Terminology","text":"<p>From the sea of technical lingo, I will mostly use three concepts (and often abuse terminology):</p> Symbol <p>A symbol is an entity that conveys meaning in a given context. It can be seen as a \"meme\" in that it represents an idea or recognized concept. For example, it can be a single character or unit of text as perceived by a human reader (regardless of the underlying primitive blocks from which it is formed). The digit <code>1</code> is a symbol, so is that letter <code>\u00e9</code>, and so is the emoji \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66.</p> Character <p>A primitive building block for symbols. It is common to refer to a visible (i.e., a user-perceived) character as a grapheme.</p> Code point <p>Unicode code points are unsigned integers<sup>7</sup> that map one to one with (primitive) characters. That is, to each character in the Unicode character set there is a corresponding integer code point as its index.</p> <p>For example, the code point <code>97</code> corresponds to the grapheme <code>e</code>. Every (primitive) character can be seen as a symbol, but the opposite is not true because there are many symbols that do not have an assigned code point. That is, some symbols are defined in terms of a sequence of characters (and thus, of code points). Such symbols are commonly referred to as grapheme clusters. An example of a grapheme cluster is \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 (as we will see shortly, it consists of 7 characters 4 of which are graphemes).</p>"},{"location":"blog/202412-python-strings/#redundancy-of-representation","title":"Redundancy of representation","text":"<pre><code>flowchart LR\n    subgraph G0 [symbol]\n        symbol{\"\u00e9\"}\n    end\n    subgraph G1 [as one code point]\n        one_code_point[\"\u00e9 (U+00E9)\"]\n    end\n    subgraph G2 [as two code points]\n        dispatch@{ shape: framed-circle }\n        dispatch --&gt; two_code_point_1[\"e (U+0065)\"]\n        dispatch --&gt; two_code_point_2[\"\u0301  (U+0301)\"]\n    end\n    symbol --&gt; dispatch\n    symbol --&gt; one_code_point\n\n    style symbol font-size:20px\n    style one_code_point font-size:18px\n    style two_code_point_1 font-size:18px\n    style two_code_point_2 font-size:18px</code></pre> <p>In Unicode, the symbol \u00e9 can be encoded in two ways (see Unicode equivalence). First, it has a dedicated code point (which defines it as a \"primitive\" grapheme). Second, it can be represented as a combination of e and an acute accent (which makes it a grapheme cluster as well).</p> <pre><code>s1 = \"\u00e9\"  # using one code point (U+00E9)\ns2 = \"e\u0301\"  # using two code points (equivalent to s2 = \"e\\u0301\")\n\nassert s1 != s2\nassert len(s1) == 1\nassert len(s2) == 2\n\nfor char in s2:\n    code_point = ord(char)\n    print(f\"{code_point} ({hex(code_point)})\")\n</code></pre> <p>Output: (1)</p> <ol> <li> <p><code>M-x describe-char</code> in <code>emacs</code> gives:</p> on \u00e9 (one code point)on e\u0301 (two code points) <pre><code>             position: 1 of 1 (0%), column: 0\n            character: \u00e9 (displayed as \u00e9) (codepoint 233, #o351, #xe9)\n              charset: iso-8859-1 (Latin-1 (ISO/IEC 8859-1))\ncode point in charset: 0xE9\n               script: latin\n               syntax: w    which means: word\n             category: .:Base, L:Strong L2R, c:Chinese, j:Japanese, l:Latin, v:Viet\n             to input: type \"C-x 8 RET e9\" or \"C-x 8 RET LATIN SMALL LETTER E WITH ACUTE\"\n          buffer code: #xC3 #xA9\n            file code: #xC3 #xA9 (encoded by coding system utf-8-unix)\n              display: terminal code #xC3 #xA9\n\nCharacter code properties: customize what to show\n  name: LATIN SMALL LETTER E WITH ACUTE\n  old-name: LATIN SMALL LETTER E ACUTE\n  general-category: Ll (Letter, Lowercase)\n  decomposition: (101 769) ('e' ' ')\n</code></pre> <pre><code>             position: 1 of 2 (0%), column: 0\n            character: e (displayed as e) (codepoint 101, #o145, #x65)\n              charset: ascii (ASCII (ISO646 IRV))\ncode point in charset: 0x65\n               script: latin\n               syntax: w    which means: word\n             category: .:Base, L:Strong L2R, a:ASCII, l:Latin, r:Roman\n             to input: type \"C-x 8 RET 65\" or \"C-x 8 RET LATIN SMALL LETTER E\"\n          buffer code: #x65\n            file code: #x65 (encoded by coding system utf-8-unix)\n              display: composed to form \"e \" (see below)\n\nComposed with the following character(s) \" \" by these characters:\n e (#x65) LATIN SMALL LETTER E\n   (#x301) COMBINING ACUTE ACCENT\n\nCharacter code properties: customize what to show\n  name: LATIN SMALL LETTER E\n  general-category: Ll (Letter, Lowercase)\n  decomposition: (101) ('e')\n</code></pre> </li> </ol> <pre><code>101 (0x65)\n769 (0x301)\n</code></pre>"},{"location":"blog/202412-python-strings/#example-a-family","title":"Example: a family","text":"<pre><code>flowchart TD\n    %%{init: {'themeVariables': {'title': 'My Flowchart Title'}}}%%\n    family1[\"\ud83d\udc69\u200d\ud83d\udc67\"]\n    family2[\"\ud83d\udc69\u200d\ud83d\udc69\u200d\ud83d\udc67\"]\n    family3[\"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66\"]\n    family4[\"\ud83d\udc6a\ufe0e\"]\n    family5[\"\ud83d\udc68\u200d\ud83d\udc66\u200d\ud83d\udc66\"]\n    C@{ shape: framed-circle, label: \"Stop\" }\n    C --&gt; cp1[\"\ud83d\udc68\"]\n    C --&gt; cp2[\"U+200d\"]\n    C --&gt; cp3[\"\ud83d\udc69\"]\n    C --&gt; cp4[\"U+200d\"]\n    C --&gt; cp5[\"\ud83d\udc67\"]\n    C --&gt; cp6[\"U+200d\"]\n    C --&gt; cp7[\"\ud83d\udc66\"]\n    family3 --&gt; C\n\n    cp1-.-&gt;cp1-hex[\"U+1f468\"]\n    cp3-.-&gt;cp3-hex[\"U+1f469\"]\n    cp5-.-&gt;cp5-hex[\"U+1f467\"]\n    cp7-.-&gt;cp7-hex[\"U+1f466\"]\n\n    style family1 font-size:50px\n    style family2 font-size:50px\n    style family3 font-size:50px\n    style family4 font-size:50px\n    style family5 font-size:50px\n    style cp1 font-size:30px\n    style cp2 font-size:30px\n    style cp3 font-size:30px\n    style cp4 font-size:30px\n    style cp5 font-size:30px\n    style cp6 font-size:30px\n    style cp7 font-size:30px\n    style cp1-hex font-size:30px\n    style cp3-hex font-size:30px\n    style cp5-hex font-size:30px\n    style cp7-hex font-size:30px</code></pre> <p>There are various emoji symbols that portray a family. They have different semantics, which is reflected by the code points used to form them. In the representation of the middle one (depicted on the lower levels), there are 4 primitive graphemes glued together with the zero-width joiner character <code>U+200d</code>. We can use <code>list(\"\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66\")</code> to get a list of characters associated with the code points that form \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66.</p>"},{"location":"blog/202412-python-strings/#indexing","title":"Indexing","text":"<p>Consider the string <code>sentense = \"This \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 is my family!\"</code>. As python strings are (stored as) sequences of code points, <code>sentense[:6]</code> would give <code>\"This \ud83d\udc68\"</code> because \ud83d\udc68 corresponds to the first (also called a base) code point of \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66. As can be expected <code>sentense[:8]</code> returns <code>\"This\ud83d\udc68\u200d\ud83d\udc69\"</code>, where the zero-width joiner is not visible<sup>1</sup>.</p> <p>The situation can get tricky with symbols that may have different Unicode representations. For example <code>len(\"L'id\u00e9e a \u00e9t\u00e9 r\u00e9\u00e9valu\u00e9e.\")</code> is 23, while <code>len(\"L'ide\u0301e a e\u0301te\u0301 re\u0301e\u0301value\u0301e.\")</code> is 29 because all symbols e\u0301 in the latter string are encoded using two code points. One can imagine strings with a mix of representations for the same symbols which can be difficult to handle in an ad hoc manner.</p>"},{"location":"blog/202412-python-strings/#grapheme-clustering","title":"Grapheme clustering","text":"<p>The Unicode standard defines rules for identifying sequences of code points that are meant to form a particular symbol (i.e., grapheme cluster). Finding symbol boundaries is a common problem e.g., in text editors and terminal emulators. As an example, consider the following functionality from the <code>grapheme</code><sup>2</sup> package:</p> <pre><code>import grapheme\n\nsentense = \"This \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc67\u200d\ud83d\udc66 is my family!\"\n\nassert len(sentense) == 26\nassert grapheme.length(sentense) == 20\nassert not grapheme.startswith(sentense, sentense[:6])\n</code></pre>"},{"location":"blog/202412-python-strings/#normalization","title":"Normalization","text":"<p>The <code>unicodedata</code> package is a part of python's standard library and can be used to normalize a string. That is, to detect symbols for which alternative Unicode encodings exist and to convert them to a given canonical form.</p> <pre><code>import unicodedata\n\ns1 = \"L'id\u00e9e a \u00e9t\u00e9 r\u00e9\u00e9valu\u00e9e.\"\nassert len(s1) == 23\n\n# each \"\u00e9\" becomes \"e\\u0301\"\ns2 = unicodedata.normalize(\"NFD\", s1) # canonical decomposition\nassert len(s2) == 29 # (1)!\n\ns3 = unicodedata.normalize(\"NFC\", s2) # canonical composition\nassert len(s3) == 23\nassert s1 == s3\nassert s1 != s2\n</code></pre> <ol> <li>While the representation of symbols resulting from the <code>NDF</code> canonical decomposition     may contain more code points, it allows for greater flexibility of text processing     in many contexts, e.g., string pattern matching.</li> </ol>"},{"location":"blog/202412-python-strings/#memory-footprint","title":"Memory footprint","text":"<p>The above discussion is mostly abstract in that it makes no assumptions on how code points (ranging from <code>0</code> to <code>1114111</code>) are to be stored in memory. Starting from PEP 393, python addresses the memory storage problem in a pragmatic way by handling four cases which depend only on one parameter: the largest code point occurring in the string.</p> <pre><code>import sys\nimport unicodedata\n\ns1 = \"L'id\u00e9e a \u00e9t\u00e9 r\u00e9\u00e9valu\u00e9e.\"\ns2 = unicodedata.normalize(\"NFD\", s1)\n\nm1, m2 = max(s1), max(s2)\nprint(f\"[s1]: {ord(m1)} ( {m1} ) #bytes = {sys.getsizeof(s1)}\")\nprint(f\"[s2]: {ord(m2)} ( {m2}  ) #bytes = {sys.getsizeof(s2)}\")\n</code></pre> <p>Output:</p> <pre><code>[s1]: 233 ( \u00e9 ) #bytes = 80\n[s2]: 769 ( \u0301  ) #bytes = 116\n</code></pre> <p>The largest code point for the <code>s2</code> string corresponds to the combining acute accent, while for the <code>s1</code> string it corresponds to <code>\u00e9</code>.</p> <p>The four cases are:</p> \\[\\begin{align}     \\texttt{code_point_bytes}(s) = \\begin{cases}         1, &amp; \\text{if $\\mu(s) &lt; 2^7$}.\\\\         1, &amp; \\text{if $\\mu(s) &lt; 2^8$}.\\\\         2, &amp; \\text{if $\\mu(s) &lt; 2^{16}$}.\\\\         4, &amp; \\text{otherwise}.     \\end{cases} \\end{align}\\] <p>where \\(\\mu(s)\\) denotes the largest code point in the string \\(s\\). The memory required to store \\(s\\) is</p> \\[ \\texttt{struct_bytes}(s) + (\\texttt{len}(s) + 1) \\cdot \\texttt{code_point_bytes}(s), \\] <p>where \\(\\texttt{len}(s)\\) is the number of code points in \\(s\\) and, the size of the <code>C-struct</code> that holds the data is given by<sup>3</sup></p> \\[\\begin{align}     \\texttt{struct_bytes}(s) = \\begin{cases}         40, &amp; \\text{if $\\mu(s) &lt; 2^7$}.\\\\         56, &amp; \\text{otherwise}.     \\end{cases} \\end{align}\\] <p>The above logic is implemented in the <code>string_bytes</code> function below<sup>4</sup>.</p> <code>def string_bytes(s):</code> <pre><code>def string_bytes(s):\n    numb_code_points, max_code_points = len(s), ord(max(s))\n\n    # C-structs in cpython/Objects/unicodeobject.c\n    # ----------------------------------------------\n    # ASCII     (use PyASCIIObject):\n    #   2 x ssize_t       = 16\n    #   6 x unsigned int  = 24\n    # otherwise (use PyCompactUnicodeObject):\n    #   1 x PyASCIIObject = 40\n    #   1 x ssize_t       = 8\n    #   1 x char *        = 8\n    # assuming a x86_64 architecture\n    struct_bytes = 56\n    if max_code_points &lt; 2**7:\n        code_point_bytes = 1\n        struct_bytes = 40\n    elif max_code_points &lt; 2**8:\n        code_point_bytes = 1\n    elif max_code_points &lt; 2**16:\n        code_point_bytes = 2\n    else:\n        code_point_bytes = 4\n\n    # `+ 1` for zero termination\n    # the result is identical with sys.getsizeof(s)\n    return struct_bytes + (numb_code_points + 1) * code_point_bytes\n</code></pre> <p>For the above example, <code>s1</code> is <code>56 + (23 + 1) * 1 = 80</code> bytes because it falls in the second case as its largest code point is 233. The string <code>s2</code>, on the other hand, falls in the third case because the acute accent has a code point above 255 (so its size is <code>56 + (29 + 1) * 2 = 116</code> bytes).</p> <p>Three clear advantages of the PEP 393 approach:</p> <ul> <li>an optimized ASCII implementation can be used for the most common (ASCII) case</li> <li>the constant number of bytes per code point<sup>6</sup> results   in constant-time indexing and facilitates other operations</li> <li>can handle natively strings containing non-BMP   characters, i.e., code points greater   than \\(2^{16} - 1\\).</li> </ul> <p>On the flip-side, concatenating a single emoji to an ASCII string increases the size x 4.</p>"},{"location":"blog/202412-python-strings/#code-units","title":"Code units","text":"<p>The building block used to actually store a code point in memory is often called a code unit. For example, consider the acute accent (<code>U+0301</code>):</p> <pre><code>flowchart TD\n    %%{init: {'themeVariables': {'title': 'My Flowchart Title'}}}%%\n\n    s[\"U+0301\"]\n    s --&gt; utf8[\"UTF-8\"]\n    s --&gt; utf16[\"UTF-16\"]\n    s --&gt; utf32[\"UTF-32\"]\n\n    C@{ shape: framed-circle, label: \"Stop\" }\n    C -.-&gt; utf8-1[\"CC\"]\n    C -.-&gt; utf8-2[\"81\"]\n\n    utf8 -.-&gt; C\n    utf16 -.-&gt; utf16-1[\"0103\"]\n    utf32 -.-&gt; utf16-2[\"01030000\"]\n\n    style utf8 stroke-width:2px,stroke-dasharray: 5 5\n    style utf16 stroke-width:2px,stroke-dasharray: 5 5\n    style utf32 stroke-width:2px,stroke-dasharray: 5 5</code></pre> <ul> <li>with a <code>utf-8</code> encoding there are two 8-bit code units (<code>0xCC</code> and <code>0x81</code>)</li> <li>with a <code>utf-16</code> encoding there is one 16-bit code unit</li> <li>with a <code>utf-32</code> encoding there is one 32-bit code unit .</li> </ul> <p>Note that, in the above example, the code units for <code>utf-16</code> and <code>utf-32</code> are stored using little-endian.</p>"},{"location":"blog/202412-python-strings/#four-string-encodings","title":"Four string encodings","text":"<p>A different encoding is used in each of the four cases discussed above.</p> <ul> <li>case 1 \\(\\left(\\mu(s) &lt; 2^7\\right)\\): ASCII (which is equivalent to UTF-8 in this range)</li> <li>case 2 \\(\\left(\\mu(s) &lt; 2^8\\right)\\): UCS1 (i.e., LATIN-1)</li> <li>case 3 \\(\\left(\\mu(s) &lt; 2^{16}\\right)\\): UCS2 (i.e., UTF-16)</li> <li>case 4 \\(\\left(\\mu(s) \\geq 2^{16}\\right)\\): UCS4 (i.e., UTF-32).</li> </ul> <p>For example, the string <code>mess</code> in the snippet below has 8 code points and \\(\\mu(\\texttt{mess}) = 65039\\), hence we are in case 3 in which UTF-16 encoding should be used. At the end, the encoding computed manually is compared<sup>5</sup> with the actual memory occupied by our string.</p> <pre><code>mess = \"I\u2665\ufe0f\u65e5\u672c\u0413\u041e\u00a9\"\n\nassert len(mess) == 8\nassert ord(max(mess)) == 65039  # case 3: 255 &lt; 65039 &lt; 65536\n\n# utf-16-le stands for utf-16 with little-endian\nencoding = b''.join([char.encode(\"utf-16-le\") for char in mess]).hex()\n\nassert string_bytes(mess) == 74  # 56 + (8 + 1) * 2\nassert len(encoding) == 32  # i.e., 16 bytes as it is in hex\nassert encoding == \"490065260ffee5652c6713041e04a900\"\n\n# --------------------------------------------------------------------------\n# compare to groundtruth (this is a hack!)\n# --------------------------------------------------------------------------\nimport ctypes\nimport sys\n\ndef memory_dump(string):\n    address = id(string)  # assuming CPython\n    buffer = (ctypes.c_char * sys.getsizeof(string)).from_address(address)\n    return bytes(buffer)\n\n# [56:] removes what we called struct_bytes above (in CPython they come first)\n# [:-2] removes the zero termination bytes\nassert memory_dump(mess)[56:-2].hex() == encoding\n# --------------------------------------------------------------------------\n</code></pre>"},{"location":"blog/202412-python-strings/#bytes-objects","title":"Bytes objects","text":"<p>As we have seen, the code units used to store a python string in memory depend on the string itself and are abstracted away from the user. While this is a good thing in many cases, sometimes we need more fine-grained control. To this end, python provides the \"bytes\" object (an immutable sequences of single bytes). Actually we already used it in the previous example as it is the return type of <code>str.encode</code>.</p> <p>Let us consider the string <code>a_man = \"a\ud83d\udc68\"</code>. By now we know that it is stored using 4 bytes per code point. Using <code>a_man.encode(\"utf-32\")</code> we obtain:</p> <ul> <li><code>\"a\"</code>: <code>97, 0, 0, 0</code></li> <li><code>\"\ud83d\udc68\"</code>: <code>104, 244, 1, 0</code>.</li> </ul> <p>If we relax the constraint of constant number of bytes per code point, we can dedicate less space to our string. Using <code>a_man.encode(\"utf-16\")</code> we obtain:</p> <ul> <li><code>\"a\"</code>: <code>97, 0</code></li> <li><code>\"\ud83d\udc68\"</code>: <code>61, 216, 104, 220</code></li> </ul> <p>or using <code>a_man.encode(\"utf-8\")</code>:</p> <ul> <li><code>\"a\"</code>: <code>97</code></li> <li><code>\"\ud83d\udc68\"</code>: <code>240, 159, 145, 168</code>.</li> </ul> <p>All above representations have their applications. For example UTF-8 provides compatibility with ASCII and efficient data storage, while UTF-16 and UTF-32 allow for faster processing of a larger range of characters. Having the possibility to easily/efficiently change representations is convenient.</p> <p>Bytes do not necessarily have to be associated with individual code points, as is the case when using <code>str.encode</code>. For example, suppose we want to express the string <code>\"a1b1\"</code> as a byte object, where each pair of characters represents a byte in hex (i.e., <code>0xA1</code> followed by <code>0xB1</code>). In this case, using <code>list(\"a1b1\".encode())</code> is not appropriate, as it would return <code>[97, 49, 98, 49]</code>, which are the ASCII codes for the characters <code>a</code>, <code>1</code>, <code>b</code>, and <code>1</code>, respectively. Instead, we should consider the additional structure and use <code>list(bytes.fromhex(\"a1b1\"))</code>, which results in <code>[161, 177]</code>.</p> <p>Bytes objects can also be used in other contexts. For instance, <code>(1).to_bytes(4, byteorder='little')</code> returns the byte representation of the integer 1 (in little-endian).</p>"},{"location":"blog/202412-python-strings/#immutability","title":"Immutability","text":"<p>The design decision to have immutable string in python has far-reaching implication related to e.g., hashing, performance optimizations, garbage collection, thread safety etc. In addition to all this, having immutable strings was a prerequisite for the approach in PEP 393.</p> <ol> <li> <p>The string might be rendered as <code>\"This \ud83d\udc68\\u200d\ud83d\udc69\"</code>.\u00a0\u21a9</p> </li> <li> <p><code>pip install grapheme</code> \u21a9</p> </li> <li> <p>Assuming a <code>x86_64</code> architecture (see the <code>string_bytes</code> function for more details).\u00a0\u21a9</p> </li> <li> <p>Based on <code>PyObject * PyUnicode_New(Py_ssize_t size, Py_UCS4 maxchar)</code> in <code>unicodeobject.c</code>.\u00a0\u21a9</p> </li> <li> <p>We used a <code>CPython</code> implementation of <code>python 3.12</code>.\u00a0\u21a9</p> </li> <li> <p>The smallest possible is always chosen.\u00a0\u21a9</p> </li> <li> <p>Often expressed as a hexadecimal number.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/202511-make-double-colon-rules/","title":"GNU Make: double-colon rules","text":""},{"location":"blog/202511-make-double-colon-rules/#gnu-make-double-colon-rules","title":"<code>GNU Make</code>: double-colon rules","text":"<p>Consider an integration testing setup where the command to test appears on the first line of a text file, followed by the expected output. I used to run all tests using:</p> <pre><code>TESTS := test-html test-latex test-semicolon ...\n\n.PHONY: $(TESTS)\n$(TESTS): COMMAND = $(shell head -n 1 $(TEST_DIR/$@))\n$(TESTS): EXPECTED = ... $(TEST_DIR/$@)\n$(TESTS): RESULT = ...\n$(TESTS):\n    compare $(EXPECTED) with $(RESULT)\n</code></pre> <p>but the other day I had the following problem: one of the tests required additional post-processing.</p> <p>Long story short: I switched to using double-colon rules:</p> <pre><code>$(TESTS):: COMMAND = ... $(TEST_DIR/$@)\n$(TESTS):: EXPECTED = ... $(TEST_DIR/$@)\n$(TESTS):: RESULT = ...\n$(TESTS)::\n    compare $(EXPECTED) with $(RESULT)\n\ntest-latex::\n    post-processing\n</code></pre> <p>The key difference from normal (single-colon) rules is that, instead of having just one, we can associate multiple recipes with a target, which are executed sequentially in the order they are defined.</p> <p>There might be downsides I am not aware of, but this seems like a convenient and extensible solution to the above problem. I no longer need to come up with new target names or reorganise tests to ensure that all post-processing steps are executed. All I have to do is \"register\" additional recipes with a target -- much like hooks -- which <code>Make</code> runs automatically whenever needed. The \"whenever needed\" part can be controlled by means of specifying dependencies to my <code>test-latex::</code> target.</p> <p>The docs say:</p> <p>Double-colon rules are somewhat obscure and not often very useful; they provide a mechanism for cases in which the method used to update a target differs depending on which prerequisite files caused the update, and such cases are rare.</p> <p>This might be true when the target is a real file but in the context of <code>.PHONY</code> targets (where the side-effect is the intended effect), double-colon rules seem to provide a useful mechanism that let me let <code>Make</code> do its job.</p> <p>And another quote from the docs:</p> <p>Double-colon rules with the same target are in fact completely separate from one another. Each double-colon rule is processed individually, just as rules with different targets are processed.</p> <p>Maybe I am parsing this the wrong way but different targets can have different target-specific variables, while target-specific variables used in the recipes of double-colon rules (associated with the same target) are shared:</p> <p></p><pre><code>t:: A = 1\nt::; @echo $(A)\nt:: A = 2\nt::; @echo $(A)\nt:: A = 3\n</code></pre> outputs<p></p> <pre><code>3\n3\n</code></pre>"},{"location":"blog/202511-makefile-doc/","title":"Documenting Makefiles","text":""},{"location":"blog/202511-makefile-doc/#documenting-makefiles","title":"Documenting Makefiles","text":"<p>After having spent years documenting my <code>Makefile</code>s using (a variation of)  this <code>awk</code> one-liner, I decided to factor  things out in a library makefile-doc. This  turned out to be a nice small project with <code>awk</code>. The following is a short overview.</p> <p>As an example, consider the documentation of the <code>Makefile</code> (at tag <code>v1.4</code>) of makefile-doc:</p> <pre>-----------------------\nAvailable targets:\n-----------------------\nhelp            Show this help\ntest            Run integration tests\nutest           Run unit tests\ntest-all        Run integration tests with all supported awk variants\nutest-all       Run unit tests with all supported awk variants\ncoverage.html   Run integration/unit tests with goawk and generate a coverage report\nlint            Lint the code using gawk\nclean-bin       Remove all downloaded awk variants\nclean           Remove coverage reports\nrelease         Create github release at latest tag\n\n------ Individual integration tests ------\n\ntest-           Recipes:\n                ---------\n                backslash-comments\n                backslash-recipe\n                backticks\n                default\n                deprecated\n                ...\n\n-----------------------\nCommand-line arguments:\n-----------------------\nAWK             Supported awk variants: {awk, mawk, nawk, bawk, wak, goawk}\nOUTPUT_FORMAT   {ansi, html}\nUPDATE_RECIPE   If set, the expected value of a test recipe is updated\n                e.g., make test-default UPDATE_RECIPE=1\n-----------------------\n</pre> <p>I find it useful to document both targets and variables. This can be done using <code>##</code>, <code>##!</code> and <code>##%</code> placed either above them or inline<sup>1</sup>. The syntax is:</p> <pre><code>##% A variable\nVAR := 1\n\n## Top-level docs take precedense\n## and could stretch over multiple lines\nt1: ## This is not shown\n\n##@ This defines a section\n\nt2: ##! This is the doc for t2\n</code></pre> <p>Explicitly listing (in an automated way) the possible values for command-line arguments is helpful. For example, currently there are 6 supported variants of awk as can be seen from the documentation of the command-line argument <code>AWK</code>. In a simmilar manner one can see how individual tests can be run, using <code>make test-</code> followed by a particular recipe (the number of entries and their formatting can be specified using several parameters).</p> <p>To use <code>makefile-doc</code>, simply place the following at the top of your <code>Makefile</code> (for more details see the project README):</p> <pre><code>help: URL := github.com/drdv/makefile-doc/releases/latest/download/makefile-doc.awk\nhelp: DIR := $(HOME)/.local/share/makefile-doc\nhelp: SCR := $(DIR)/makefile-doc.awk\nhelp: ## show this help\n    @test -f $(SCR) || wget -q -P $(DIR) $(URL)\n    @awk $(VFLAGS) -f $(SCR) $(MAKEFILE_LIST)\n</code></pre> <p>The variable <code>VFLAGS</code> can be left empty or can be used to pass options to the <code>makefile-doc.awk</code> script via the standard <code>-v OPTION=VALUE</code> flag of <code>awk</code>. This is pretty much it, the rest is exploring some of the options and finding what works for you. As an example, the project's <code>Makefile</code> uses the following options:</p> <pre><code>help: VFLAGS := \\\n    -v SUB='$(TESTS_SUB);$(AWK_SUB)' \\\n    -v EXPORT_THEME=$(EXPORT_THEME) \\\n    -v COLOR_BACKTICKS=33 \\\n    -v OUTPUT_FORMAT=$(OUTPUT_FORMAT)`\n</code></pre> <p>The above HTML docs have been generated using <code>make OUTPUT_FORMAT=HTML</code> (output to Latex is supported as well).</p>"},{"location":"blog/202511-makefile-doc/#substitutions","title":"Substitutions","text":"<p>The <code>SUB</code> option specifies substitutions and value formatting. For example, the listing of the supported <code>awk</code> variants is generated using</p> <pre><code>AWK_SUB := &lt;L:0,M:0,I:{,T:},S:\\\\,&gt;AWK:$(SUPPORTED_AWK_VARIANTS)\n</code></pre> <p>The actual values come from the variable <code>$(SUPPORTED_AWK_VARIANTS)</code>, while the following parameters control the formatting:</p> <ul> <li><code>L:0</code> - list the values starting from the current line</li> <li><code>M:0</code> - list the values on a single line (as opposed to <code>M:1</code> for multi-line listing)</li> <li><code>I:{</code> and <code>T:}</code> define the initial and terminal delimiters</li> <li><code>S:\\\\,</code> is the separator (note that the comma has to be escaped).</li> </ul> <p>The format of a substitution is <code>[&lt;p1:v1,...&gt;]NAME[:LABEL]:[VALUES]</code>. For the above example we have:</p> <ul> <li><code>[&lt;p1:v1,...&gt;]</code> <code>&lt;L:0,M:0,I:{,T:},S:\\\\,&gt;</code></li> <li><code>NAME</code> <code>AWK</code></li> <li><code>[VALUES]</code> <code>$(SUPPORTED_AWK_VARIANTS)</code>.</li> </ul> <p>Only the optional <code>[:LABEL]</code> is omitted. It is normally used to rename targets specified in terms of variables -- as is the case with the individual test targets above (where <code>[:LABEL]</code> <code>test-</code> because the actual target name is <code>$(TESTS):</code> and it is not very informative at the level of the documentation).</p>"},{"location":"blog/202511-makefile-doc/#theme","title":"Theme","text":"<p>The default Solarized theme can be easily customised. Below is an example of the same Makefile but using the Dracula theme and exported to png<sup>2</sup>. Unlike the HTML version above (where the background is left to be transparent), here we have set a black background. A \"standalone\" image is generated for convenient use in presentations.</p> <p></p> <ol> <li> <p>The meaning of the three tokens is entirely up to you.\u00a0\u21a9</p> </li> <li> <p>This is done by specifying the <code>EXPORT_THEME</code> option (see the project README.)\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/202512-make-adding-builtin-function/","title":"GNU Make: Adding a built-in function","text":""},{"location":"blog/202512-make-adding-builtin-function/#gnu-make-adding-a-built-in-function","title":"<code>GNU Make</code>: Adding a built-in function","text":"<p>I saw this question on the help-make mailing list and I decided to see how easy it would be to register a new built-in function in GNU Make. It turned out to be quite easy (in comparison, it took me more time to write this post). I then found out that there is a section \"Anatomy of a Built-In Function\" in \"The GNU Make Book\" by John Graham-Cumming which helped me to see alternatives.</p>"},{"location":"blog/202512-make-adding-builtin-function/#the-sources","title":"The sources","text":"<pre><code>curl -O https://ftp.gnu.org/gnu/make/make-4.4.tar.gz &amp;&amp; tar xvf make-4.4.tar.gz\ncd make-4.4 &amp;&amp; ./configure &amp;&amp; make\n</code></pre>"},{"location":"blog/202512-make-adding-builtin-function/#the-function","title":"The function","text":"<pre><code>static char *\nfunc_location (char *o, char **argv, const char *funcname UNUSED)\n{\n  const floc *flocp = reading_file;\n  if (flocp &amp;&amp; flocp-&gt;filenm) {\n    const char *mode = argv[0];\n    if (!strcmp(mode, \"file\")) {\n      return variable_buffer_output(o, flocp-&gt;filenm, strlen(flocp-&gt;filenm));\n    } else if (!strcmp(mode, \"line\")) {\n      char buf[21];\n      snprintf(buf, sizeof(buf), \"%lu\", flocp-&gt;lineno + flocp-&gt;offset);\n      return variable_buffer_output(o, buf, strlen(buf));\n    }\n  }\n  return o;\n}\n</code></pre> <p>All built-in functions have the same signature:</p> <ul> <li><code>o</code>: a pointer to the buffer where the result of the macro expansion would be substituted</li> <li><code>argv</code>: an array of arguments</li> <li><code>funcname</code>: a label (which we don't use).</li> </ul> <p>The logic of the code is not important<sup>1</sup> -- a string containing filename/line number is formed and <code>variable_buffer_output(...)</code> is called to insert it into the output buffer. We then add <code>func_location</code> to <code>src/function.c</code> and include it into a hash table of built-in functions (where we specify that there would be exactly one argument which would be expanded as per usual):</p> <pre><code>static struct function_table_entry function_table_init[] =\n{\n /*         Name            MIN MAX EXP? Function */\n  FT_ENTRY (\"abspath\",       0,  1,  1,  func_abspath),\n  // ...\n  FT_ENTRY (\"location\",      1,  1,  1,  func_location),\n};\n</code></pre> <p>Then we run <code>make</code> in <code>make-4.4</code> again and we are done.</p>"},{"location":"blog/202512-make-adding-builtin-function/#the-example","title":"The example","text":"<p>So now we have a function that takes one argument (either <code>file</code> of <code>line</code>). We define two recursively expanded variables (<code>__FILE__</code> and <code>__LINE__</code>) for convenience:</p> Makefile.test<pre><code>__FILE__ = $(location file)\n__LINE__ = $(location line)\n\n.PHONY: all tell-lines\n\nall: tell-lines other\n\n# empty/commented lines before the first recipe line are accounted for\n    @echo \"[one logical line  ] $(__FILE__):$(__LINE__)\" \\\n    \"($(__LINE__))\"\n\n# all other empty/commented lines in a recipe are ignored\n    @echo \"[after empty lines ] $(__FILE__):$(__LINE__)\"\n\n# the line number of the first recipe line is correct\ntell-lines:\n    @printf \"tell-lines at %s:%d\\n\" $(__FILE__) $(__LINE__)\n\ninclude Makefile.other\n</code></pre> Makefile.other<pre><code>.PHONY: other\nother:\n    @echo \"[from included file] $(__FILE__):$(__LINE__)\"\n</code></pre> <p>Running <code>make -f Makefile.test</code> outputs: </p><pre><code>tell-lines at Makefile.test:17\n[from included file] Makefile.other:3\n[one logical line  ] Makefile.test:9 (9)\n[after empty lines ] Makefile.test:10\n</code></pre><p></p> <p>As noted in the comments above, the line numbers indicate \"logical lines\" and empty/commented lines after the first recipe line are not accounted for (my guess is that this is a consequence of \"how makefiles are parsed\").</p> <ol> <li> <p>I am sure I don't handle some edge-cases -- but that's beyond the point.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/202512-make-double-expand/","title":"GNU Make: double-expand","text":""},{"location":"blog/202512-make-double-expand/#gnu-make-double-expand","title":"<code>GNU Make</code>: double-expand","text":"<p>A few days ago I stumbled upon this article late in the evening and it kept me awake the whole night! It describes a cool trick that allows to \"double-expand\" a macro without using <code>eval</code>. The following is a summary of the main idea, a reflection of why it works and a small extension.</p>"},{"location":"blog/202512-make-double-expand/#macro-expansion","title":"Macro expansion","text":"<p>A macro is a string that expands to another string when referenced. For example the macro <code>foo</code>, can be expanded using <code>$(foo)</code>. That is, the string <code>$(foo)</code> would be replaced by the result of an expansion process. A macro reference takes the form of a dollar sign followed by either a single character or a string enclosed in matching parenthesis, e.g., <code>$foo</code> is the same as <code>$(f)oo</code> which is different from <code>$(foo)</code>. Normally, each <code>$</code> (that defines the start of a macro reference) expands exactly<sup>1</sup> one string in a left-to-right depth-first traversal.</p> <p>Apart from a string substitution, a macro expansion may result in a side-effect, as is the case, e.g., with <code>$(info ...)</code>, which prints its expanded argument(s) on standard output and returns an empty string. We could think of <code>info</code> as a built-in function. In addition to built-in functions, there is the notion of user-defined functions. Next, I make an important distinction between the two.</p>"},{"location":"blog/202512-make-double-expand/#functions","title":"Functions","text":"<p>A macro reference of the form <code>$(string srg1,arg2,...)</code> is interpreted as a built-in function call, when <code>string</code> exists as a key in a hash table of built-in functions. In such a case, the following comma-separated strings are interpreted as positional arguments and are either all expanded as per normal, or are left to the function to expand in a custom way. In the <code>C</code> code, this is governed by the <code>EXP?</code> column in</p> <pre><code>static struct function_table_entry function_table_init[] =\n{\n /*         Name            MIN MAX EXP? Function */\n  FT_ENTRY (\"abspath\",       0,  1,  1,  func_abspath),\n  FT_ENTRY (\"addprefix\",     2,  2,  1,  func_addsuffix_addprefix),\n  // ...\n  FT_ENTRY (\"foreach\",       3,  3,  0,  func_foreach),\n  FT_ENTRY (\"let\",           3,  3,  0,  func_let),\n  FT_ENTRY (\"call\",          1,  0,  1,  func_call),\n  // ...\n  FT_ENTRY (\"intcmp\",        2,  5,  0,  func_intcmp),\n  FT_ENTRY (\"if\",            2,  3,  0,  func_if),\n  FT_ENTRY (\"or\",            1,  0,  0,  func_or),\n  FT_ENTRY (\"and\",           1,  0,  0,  func_and),\n  FT_ENTRY (\"value\",         0,  1,  1,  func_value),\n};\n</code></pre> <p>Except for <code>foreach</code>, <code>let</code>, <code>intcmp</code>, <code>if</code>, <code>or</code> and <code>and</code>, all built-in functions have their arguments expanded.</p> <p>A user-defined function, on the other hand, is always expanded as a standard macro that could, potentially, contain some parameters/arguments to be specified later. For example:</p> <pre><code>a = a\nb = function\n\nmy-function = This is $a custom $b.\n\n$(info $(my-function))\n$(info $(let a b,my macro,$(my-function)))\n\nall:;@:\n</code></pre> <p>would output</p> <pre><code>This is a custom function.\nThis is my custom macro.\n</code></pre> <p>Instead of <code>a</code> and <code>b</code> as in the above example, normally we name parameters as <code>1</code>, <code>2</code>, etc. and use the built-in <code>$(call ...)</code> function:</p> <pre><code>my-standard-function = This is $1 standard $2.\n$(info $(call my-standard-function,a,function))\n</code></pre>"},{"location":"blog/202512-make-double-expand/#the-call-function","title":"The <code>call</code> function","text":"<p>As with most built-in functions, all arguments of <code>call</code> are expanded (see <code>function_table_init</code>). Then it is responsible for two things:</p> <ul> <li>detect if what is to be called is actually a built-in function -- in which case it is   executed directly with the already expanded arguments;</li> <li>if instead, a user-defined function is to be called, the parameters <code>1, 2, ...</code> are   initialised on a local stack and the macro is expanded (something like the above   <code>$(let ...)</code> example).</li> </ul>"},{"location":"blog/202512-make-double-expand/#double-expansion","title":"Double-expansion","text":"<p>The trick is to use <code>$(call ...)</code> to call one of the 6 builtin functions that expand their own arguments (i.e., for which <code>EXP? = 0</code>). For example:</p> <pre><code>key = value\nx = $$(key)\n\nmy-function = $(eval _tmp:=$1)$(_tmp)\n\n$(info $(call or,$x))\n$(info $(call firstword,$x))\n$(info $(call my-function,$x))\n\nall:;@:\n</code></pre> <p>would output</p> <pre><code>value\n$(key)\nvalue\n</code></pre> <p>In all cases, the argument <code>$x</code> is expanded to <code>$(key)</code>. Then <code>or</code> expands <code>$(key)</code> (as it would normally do if we call directly <code>$(or $(key))</code>) which results in <code>value</code>. <code>firstword</code>, on the other hand, \"knows\" that its argument is already expanded and simply returns the first word (which happens to be the literal string <code>$(key)</code>). To achieve the same double-expansion with a user-defined function, we would have to use <code>eval</code> and define a global variable (as in <code>my-function</code>).</p>"},{"location":"blog/202512-make-double-expand/#anonymous-functions","title":"Anonymous functions","text":"<p>Here it is worth reading the original article. It presents a nice sequence of examples that rely on the double-expansion trick to define anonymous functions. Here I present one of them<sup>2</sup> in order to point out that, as an alternative to the positional arguments in their \"Hack #3\", we could use key-value arguments, that avoid the need to define an <code>apply</code> function.</p>"},{"location":"blog/202512-make-double-expand/#fold-left","title":"Fold left","text":"<p>Let us define <code>foldl</code> similar to the following example in Racket</p> <pre><code>(define (foldl op initial sequence)\n  (if (null? sequence)\n      initial\n      (foldl op\n             (op initial (car sequence))\n             (cdr sequence))))\n</code></pre> <p>The result is:</p> <pre><code>car = $(firstword $1)\ncdr = $(wordlist 2,$(words $1),$1)\n\nfoldl = $(if $3,$\\\n            $(call foldl,$\\\n                $1,$\\\n                $(let a,$2,$\\\n                    $(let e,$(call car,$3),$\\\n                        $(call or,$1))),$\\\n                $(call cdr,$3)),$\\\n            $2)\n\n$(info $(call foldl,$$a$$e$$a,.,a b c))\n\nall:;@:\n</code></pre> <p>While this wouldn't win a code readability contest, it is kind of nice. Note how, after the initial expansion, argument <code>1</code> of <code>foldl</code> is equal to <code>$a$e$a</code> -- which is what <code>or</code> further expands using the local variables <code>$a</code> (the accumulator) and <code>$e</code> (the current element) defined in the two nested <code>let</code> blocks. The output is:</p> <pre><code>.a.b.a.c.a.b.a.\n</code></pre> <p>Our anonymous function <code>$$a$$e$$a</code> takes two parameters, which are now fixed to be <code>a</code> and <code>e</code>. The reason for using two nested <code>let</code> blocks instead of the more readable</p> <pre><code>$(let a e,$2 $(call car,$3),$(call or,$1))\n</code></pre> <p>is that the latter option doesn't work when there are spaces in the anonymous function (e.g., <code>$$a $$e $$a</code>) -- this is due to the way lists are defined in <code>Make</code>.</p>"},{"location":"blog/202512-make-double-expand/#the-6-special-functions","title":"The 6 special functions","text":"<p>As I mentioned, we could use any of the 6 special functions to implement the double-expansion trick:</p> <pre><code>key = value\nx = $$(key)\n\n$(info $(call or,$x))\n$(info $(call and,$x))\n$(info $(call if,1,$x))\n$(info $(call foreach,,_,$x))\n$(info $(call let,,,$x))\n$(info $(call intcmp,1,2,$x))\n\nall:;@:\n</code></pre> <p>The output is:</p> <pre><code>value\nvalue\nvalue\nvalue\nvalue\nvalue\n</code></pre> <p>I believe the following sentence in the docs refers to the above behaviour:</p> <p>The 'call' function expands the PARAM arguments before assigning them to temporary variables.  This means that VARIABLE values containing references to built-in functions that have special expansion rules, like 'foreach' or 'if', may not work as you expect.</p> <ol> <li> <p>But there are exceptions, which are discussed below. Note that <code>$$</code> is equivalent to <code>$($)</code>, and only the first <code>$</code> defines the start of a macro reference, while the second <code>$</code> is a part of the associated string (which expands to itself).\u00a0\u21a9</p> </li> <li> <p>The others can be modified in a similar way.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2025/#2025","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2024/#2024","title":"2024","text":""},{"location":"blog/category/gnu-make/","title":"GNU Make","text":""},{"location":"blog/category/gnu-make/#gnu-make","title":"GNU Make","text":""},{"location":"blog/category/python/","title":"python","text":""},{"location":"blog/category/python/#python","title":"python","text":""},{"location":"blog/category/sports/","title":"sports","text":""},{"location":"blog/category/sports/#sports","title":"sports","text":""}]}